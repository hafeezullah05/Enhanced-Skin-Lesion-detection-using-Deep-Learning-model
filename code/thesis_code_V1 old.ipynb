{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from natsort import natsorted # type: ignore\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import shutil\n",
    "from sklearn.metrics import precision_score as skl_precision_score\n",
    "from sklearn.metrics import recall_score as skl_recall_score\n",
    "from sklearn.metrics import f1_score as skl_f1_score\n",
    "\n",
    "from custom_dataset import CustomMelanomaDataset  # Import the custom dataset\n",
    "from resnet_model import ResNetModel\n",
    "\n",
    "\n",
    "import shutil\n",
    "#import torch.profiler\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "#from torch.profiler import profile, ProfilerActivity\n",
    "#import tkinter as tk\n",
    "#from tkinter import filedialog, messagebox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
      "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
      "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
      "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
      "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
      "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
      "\n",
      "  diagnosis benign_malignant  target  \n",
      "0   unknown           benign     0.0  \n",
      "1   unknown           benign     0.0  \n",
      "2     nevus           benign     0.0  \n",
      "3   unknown           benign     0.0  \n",
      "4   unknown           benign     0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33126 entries, 0 to 33125\n",
      "Data columns (total 8 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   image_name                     33126 non-null  object \n",
      " 1   patient_id                     33126 non-null  object \n",
      " 2   sex                            33061 non-null  object \n",
      " 3   age_approx                     33058 non-null  float64\n",
      " 4   anatom_site_general_challenge  32599 non-null  object \n",
      " 5   diagnosis                      33126 non-null  object \n",
      " 6   benign_malignant               33126 non-null  object \n",
      " 7   target                         33125 non-null  float64\n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 2.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the dynamic root path\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "root_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Display the structure of the dataset\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the RAW CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'anatom_site_general_challenge': ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'nan'\n",
      " 'lower extremityi wrote to him' 'palms/soles' 'oral/genital']\n",
      "Rows with variations of 'unknown':\n",
      " Empty DataFrame\n",
      "Columns: [image_name, patient_id, sex, age_approx, anatom_site_general_challenge, diagnosis, benign_malignant, target]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure 'anatom_site_general_challenge' is treated as a string\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].astype(str)\n",
    "\n",
    "# Strip extra spaces\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].str.strip()\n",
    "\n",
    "# Print unique values to identify any anomalies\n",
    "unique_values = df['anatom_site_general_challenge'].unique()\n",
    "print(\"Unique values in 'anatom_site_general_challenge':\", unique_values)\n",
    "\n",
    "# Check for rows that may contain 'unknown' in any form\n",
    "unknown_variations = df[df['anatom_site_general_challenge'].str.contains('unknown', case=False, na=False, regex=True)]\n",
    "print(\"Rows with variations of 'unknown':\\n\", unknown_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic entries:\n",
      " Empty DataFrame\n",
      "Columns: [image_name, patient_id, sex, age_approx, anatom_site_general_challenge, diagnosis, benign_malignant, target]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Identify rows with potentially problematic entries\n",
    "problematic_entries = df[df['anatom_site_general_challenge'].str.contains('unknown', case=False, na=False, regex=False)]\n",
    "print(\"Problematic entries:\\n\", problematic_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'unknown' values after cleanup: 527\n",
      "Unique values in 'anatom_site_general_challenge' after cleanup: ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'unknown'\n",
      " 'palms/soles' 'oral/genital']\n"
     ]
    }
   ],
   "source": [
    "# Replace common variations with 'unknown'\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].replace({\n",
    "    'nan': 'unknown',\n",
    "    'lower extremityi wrote to him': 'lower extremity',\n",
    "    # Add other replacements if needed\n",
    "})\n",
    "\n",
    "# Check again for 'unknown' values\n",
    "unknown_values = (df['anatom_site_general_challenge'] == 'unknown').sum()\n",
    "print(\"Number of 'unknown' values after cleanup:\", unknown_values)\n",
    "\n",
    "# Verify unique values after cleanup\n",
    "unique_values = df['anatom_site_general_challenge'].unique()\n",
    "print(\"Unique values in 'anatom_site_general_challenge' after cleanup:\", unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anatom_site_general_challenge\n",
      "0                      head/neck\n",
      "1                upper extremity\n",
      "2                lower extremity\n",
      "3                      head/neck\n",
      "4                upper extremity\n",
      "..                           ...\n",
      "85                     head/neck\n",
      "86                         torso\n",
      "87               lower extremity\n",
      "88               upper extremity\n",
      "89                         torso\n",
      "\n",
      "[90 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[['anatom_site_general_challenge']].head(90))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the anomaly in the original CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in each column:\n",
      " image_name                        0\n",
      "patient_id                        0\n",
      "sex                              65\n",
      "age_approx                       68\n",
      "anatom_site_general_challenge     0\n",
      "diagnosis                         0\n",
      "benign_malignant                  0\n",
      "target                            1\n",
      "dtype: int64\n",
      "Infinity values in each column:\n",
      " image_name                       0\n",
      "patient_id                       0\n",
      "sex                              0\n",
      "age_approx                       0\n",
      "anatom_site_general_challenge    0\n",
      "diagnosis                        0\n",
      "benign_malignant                 0\n",
      "target                           0\n",
      "dtype: int64\n",
      "Number of 'unknown' values in 'anatom_site_general_challenge': 527\n",
      "Unique values in 'anatom_site_general_challenge': ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'unknown'\n",
      " 'palms/soles' 'oral/genital']\n",
      "Rows with variations of 'unknown':\n",
      "          image_name  patient_id     sex  age_approx  \\\n",
      "33     ISIC_0086462  IP_3200260  female        30.0   \n",
      "61     ISIC_0099474  IP_3057277    male        45.0   \n",
      "188    ISIC_0174903  IP_2760044    male        40.0   \n",
      "200    ISIC_0178744  IP_4248414    male        25.0   \n",
      "305    ISIC_0204994  IP_5549010  female        30.0   \n",
      "...             ...         ...     ...         ...   \n",
      "32898  ISIC_9928421  IP_0961415    male        45.0   \n",
      "33001  ISIC_9963692  IP_6017204  female        35.0   \n",
      "33025  ISIC_9971473  IP_1005683    male        70.0   \n",
      "33041  ISIC_9975949  IP_9245079    male        40.0   \n",
      "33112  ISIC_9997221  IP_6353955    male        55.0   \n",
      "\n",
      "      anatom_site_general_challenge diagnosis benign_malignant  target  \n",
      "33                          unknown   unknown           benign     0.0  \n",
      "61                          unknown   unknown           benign     0.0  \n",
      "188                         unknown   unknown           benign     0.0  \n",
      "200                         unknown   unknown           benign     0.0  \n",
      "305                         unknown     nevus           benign     0.0  \n",
      "...                             ...       ...              ...     ...  \n",
      "32898                       unknown   unknown           benign     0.0  \n",
      "33001                       unknown   unknown           benign     0.0  \n",
      "33025                       unknown   unknown           benign     0.0  \n",
      "33041                       unknown     nevus           benign     0.0  \n",
      "33112                       unknown   unknown           benign     0.0  \n",
      "\n",
      "[527 rows x 8 columns]\n",
      "Maximum values in each column (excluding 'diagnosis'):\n",
      " age_approx    90.0\n",
      "target         1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define the path to CSV file\n",
    "csv_path = os.path.join(os.path.expanduser('~'), 'Desktop', 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure 'anatom_site_general_challenge' is treated as a string\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].astype(str)\n",
    "\n",
    "# Strip extra spaces\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].str.strip()\n",
    "\n",
    "# Replace common variations with 'unknown'\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].replace({\n",
    "    'nan': 'unknown',\n",
    "    'lower extremityi wrote to him': 'lower extremity',\n",
    "    # Add other replacements if needed\n",
    "})\n",
    "\n",
    "# Check for NaN values\n",
    "nan_values = df.isna().sum()\n",
    "print(\"NaN values in each column:\\n\", nan_values)\n",
    "\n",
    "# Check for infinity values\n",
    "infinity_values = df.replace([np.inf, -np.inf], np.nan).isna().sum() - df.isna().sum()\n",
    "print(\"Infinity values in each column:\\n\", infinity_values)\n",
    "\n",
    "# Check for 'unknown' values in 'anatom_site_general_challenge' column\n",
    "unknown_values = (df['anatom_site_general_challenge'] == 'unknown').sum()\n",
    "print(\"Number of 'unknown' values in 'anatom_site_general_challenge':\", unknown_values)\n",
    "\n",
    "# Print unique values to identify variations\n",
    "unique_values = df['anatom_site_general_challenge'].unique()\n",
    "print(\"Unique values in 'anatom_site_general_challenge':\", unique_values)\n",
    "\n",
    "# Check for any variations of 'unknown'\n",
    "unknown_variations = df[df['anatom_site_general_challenge'].str.contains('unknown', case=False, na=False)]\n",
    "print(\"Rows with variations of 'unknown':\\n\", unknown_variations)\n",
    "\n",
    "# Check for unusually large values\n",
    "# Exclude 'diagnosis' column from max value check\n",
    "columns_for_max = [col for col in df.columns if col != 'diagnosis']\n",
    "max_values = df[columns_for_max].max(numeric_only=True)  # Ensure numeric_only=True to avoid warnings\n",
    "print(\"Maximum values in each column (excluding 'diagnosis'):\\n\", max_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the unknown, missing, NaN values from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed CSV saved to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the base directory and paths\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "new_csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure 'anatom_site_general_challenge' is treated as a string and strip extra spaces\n",
    "df['anatom_site_general_challenge'] = df['anatom_site_general_challenge'].astype(str).str.strip()\n",
    "\n",
    "# Replace literal 'nan' strings with actual NaN values\n",
    "df.replace('nan', np.nan, inplace=True)\n",
    "\n",
    "# Remove rows with 'unknown' in 'anatom_site_general_challenge' column\n",
    "df = df[df['anatom_site_general_challenge'] != 'unknown']\n",
    "\n",
    "# Remove rows with NaN or infinity values in any column\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Drop the 'diagnosis' column\n",
    "if 'diagnosis' in df.columns:\n",
    "    df = df.drop(columns=['diagnosis'])\n",
    "\n",
    "# Check if the new CSV file already exists\n",
    "if os.path.exists(new_csv_path):\n",
    "    # Clear the existing file\n",
    "    open(new_csv_path, 'w').close()\n",
    "\n",
    "# Save the cleaned DataFrame to the new CSV file\n",
    "df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed CSV saved to {new_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying the preprocessing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in each column:\n",
      " image_name                       0\n",
      "patient_id                       0\n",
      "sex                              0\n",
      "age_approx                       0\n",
      "anatom_site_general_challenge    0\n",
      "benign_malignant                 0\n",
      "target                           0\n",
      "dtype: int64\n",
      "Infinity values in each column:\n",
      " image_name                       0\n",
      "patient_id                       0\n",
      "sex                              0\n",
      "age_approx                       0\n",
      "anatom_site_general_challenge    0\n",
      "benign_malignant                 0\n",
      "target                           0\n",
      "dtype: int64\n",
      "\n",
      "Number of 'unknown' values in 'anatom_site_general_challenge': 0\n",
      "\n",
      "Unique values in 'anatom_site_general_challenge': ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'palms/soles'\n",
      " 'oral/genital']\n",
      "\n",
      "Rows with variations of 'unknown':\n",
      " Empty DataFrame\n",
      "Columns: [image_name, patient_id, sex, age_approx, anatom_site_general_challenge, benign_malignant, target]\n",
      "Index: []\n",
      "\n",
      "Maximum values in each column (excluding 'diagnosis'):\n",
      " age_approx    90.0\n",
      "target         1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Re-load the cleaned CSV\n",
    "df_cleaned = pd.read_csv(new_csv_path)\n",
    "\n",
    "# Ensure 'anatom_site_general_challenge' is treated as a string\n",
    "df_cleaned['anatom_site_general_challenge'] = df_cleaned['anatom_site_general_challenge'].astype(str).str.strip()\n",
    "\n",
    "\n",
    "# Check for NaN values\n",
    "nan_values = df_cleaned.isna().sum()\n",
    "print(\"NaN values in each column:\\n\", nan_values)\n",
    "\n",
    "# Check for infinity values\n",
    "infinity_values = df_cleaned.replace([np.inf, -np.inf], np.nan).isna().sum() - df.isna().sum()\n",
    "print(\"Infinity values in each column:\\n\", infinity_values)\n",
    "\n",
    "# Check for 'unknown' values in 'anatom_site_general_challenge' column\n",
    "unknown_values = (df_cleaned['anatom_site_general_challenge'] == 'unknown').sum()\n",
    "print(\"\\nNumber of 'unknown' values in 'anatom_site_general_challenge':\", unknown_values)\n",
    "\n",
    "# Print unique values to identify variations\n",
    "unique_values = df_cleaned['anatom_site_general_challenge'].unique()\n",
    "print(\"\\nUnique values in 'anatom_site_general_challenge':\", unique_values)\n",
    "\n",
    "# Check for any variations of 'unknown'\n",
    "unknown_variations = df_cleaned[df_cleaned['anatom_site_general_challenge'].str.contains('unknown', case=False, na=False)]\n",
    "print(\"\\nRows with variations of 'unknown':\\n\", unknown_variations)\n",
    "\n",
    "# Check for unusually large values\n",
    "# Exclude 'diagnosis' column from max value check\n",
    "columns_for_max = [col for col in df_cleaned.columns if col != 'diagnosis']\n",
    "max_values = df_cleaned[columns_for_max].max(numeric_only=True)  # Ensure numeric_only=True to avoid warnings\n",
    "print(\"\\nMaximum values in each column (excluding 'diagnosis'):\\n\", max_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by split of NEW CSV - Split by Patient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the universal path handling logic\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "SPLIT_CSV_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv')\n",
    "\n",
    "# Universal Path Setup for Images\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'Train_JPEG', 'JPEG')\n",
    "\n",
    "# Paths for Train/Test CSVs\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_split.csv')\n",
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "CSV_PATH = os.path.join(root_path, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "new_csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv')\n",
    "\n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez',  'Thesis_Code/Enhanced-Skin-Lesion-detection-using-Deep-Learning-model/results', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (24300, 7)\n",
      "Test set size: (8230, 7)\n",
      "Train and test CSV files have been successfully written to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/split_csv.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the directory exists\n",
    "if not os.path.exists(SPLIT_CSV_DIR):\n",
    "    os.makedirs(SPLIT_CSV_DIR)\n",
    "\n",
    "# Define paths for train and test CSV files\n",
    "train_csv_path = os.path.join(SPLIT_CSV_DIR, 'train_split.csv')\n",
    "test_csv_path = os.path.join(SPLIT_CSV_DIR, 'test_split.csv')\n",
    "\n",
    "# Check if train_split.csv and test_split.csv exist; if not, create them\n",
    "if not os.path.exists(train_csv_path):\n",
    "    open(train_csv_path, 'w').close()  # Create the file if it doesn't exist\n",
    "\n",
    "if not os.path.exists(test_csv_path):\n",
    "    open(test_csv_path, 'w').close()  # Create the file if it doesn't exist\n",
    "\n",
    "# Step 2: Split the dataset by `patient_id`, ensuring no patient data is split across train and test sets\n",
    "# Group by patient_id\n",
    "grouped = df_cleaned.groupby('patient_id')\n",
    "\n",
    "# Extract unique patient_ids\n",
    "unique_patient_ids = df_cleaned['patient_id'].unique()\n",
    "\n",
    "# Split the unique patient ids into 75% train and 25% test\n",
    "train_patient_ids, test_patient_ids = train_test_split(unique_patient_ids, test_size=0.25, random_state=42)\n",
    "\n",
    "# Filter the dataframe to create train and test datasets\n",
    "train_df = df_cleaned[df_cleaned['patient_id'].isin(train_patient_ids)]\n",
    "test_df = df_cleaned[df_cleaned['patient_id'].isin(test_patient_ids )]\n",
    "\n",
    "# Display the size of the splits\n",
    "print(f\"Training set size: {train_df.shape}\")\n",
    "print(f\"Test set size: {test_df.shape}\")\n",
    "\n",
    "# Step 3: Function to check, clear, and write CSV\n",
    "def write_csv(dataframe, path):\n",
    "    if os.path.exists(path):\n",
    "        # If CSV exists, remove its contents\n",
    "        with open(path, 'w') as f:\n",
    "            pass  # Clearing the file\n",
    "    # Write the new data to the CSV\n",
    "    dataframe.to_csv(path, index=False)\n",
    "\n",
    "# Step 4: Write to train and test CSV\n",
    "write_csv(train_df, train_csv_path)\n",
    "write_csv(test_df, test_csv_path)\n",
    "\n",
    "print(f\"Train and test CSV files have been successfully written to {SPLIT_CSV_DIR}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the train set: 24300\n",
      "Total number of images in the test set: 8230\n",
      "Total unique patients in the train set: 1538\n",
      "Total unique patients in the test set: 513\n"
     ]
    }
   ],
   "source": [
    "# Print the number of images in the train and test datasets\n",
    "print(f\"Total number of images in the train set: {len(train_df['image_name'])}\")\n",
    "print(f\"Total number of images in the test set: {len(test_df['image_name'])}\")\n",
    "\n",
    "# Optionally, you can also check the number of unique patient IDs in each split:\n",
    "print(f\"Total unique patients in the train set: {train_df['patient_id'].nunique()}\")\n",
    "print(f\"Total unique patients in the test set: {test_df['patient_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNetModel(nn.Module):\n",
    "    def __init__(self, num_metadata_features):\n",
    "        super(CustomResNetModel, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained ResNet-18 model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the fully connected layer to accommodate the output of ResNet\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()  # Remove the original classification layer\n",
    "        \n",
    "        # Define additional layers for metadata processing\n",
    "        self.metadata_fc = nn.Sequential(\n",
    "            nn.Linear(num_metadata_features, 64),  # Example hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),  # Another hidden layer\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combine features from ResNet and metadata\n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(in_features + 32, 64),  # Combine ResNet features and metadata features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output layer for binary classification\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, metadata):\n",
    "        # Pass images through the ResNet model\n",
    "        resnet_features = self.resnet(images)\n",
    "        \n",
    "        # Process metadata\n",
    "        metadata_features = self.metadata_fc(metadata)\n",
    "        \n",
    "        # Concatenate ResNet features and metadata features\n",
    "        combined_features = torch.cat((resnet_features, metadata_features), dim=1)\n",
    "        \n",
    "        # Pass through the final fully connected layers\n",
    "        output = self.combined_fc(combined_features)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transforms for image augmentation and normalization\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size (224x224)\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally for data augmentation\n",
    "    transforms.RandomVerticalFlip(),    # Randomly flip the image vertically\n",
    "    transforms.ToTensor(),  # Convert PIL Image or numpy.ndarray to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images with pre-defined mean and std\n",
    "])\n",
    "\n",
    "# Define transformations for testing\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size (224x224)\n",
    "    transforms.ToTensor(),  # Convert PIL Image or numpy.ndarray to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images with pre-defined mean and std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "# Create datasets\n",
    "train_dataset = CustomMelanomaDataset(\n",
    "    csv_file=train_csv_path,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False  # Indicates that this dataset is for training\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = CustomMelanomaDataset(\n",
    "    csv_file=test_csv_path,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=test_transforms,\n",
    "    is_test=True  # Indicates that this dataset is for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "lr = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "num_workers=0\n",
    "\n",
    "# Determine if CUDA is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloaders, lossFunc, Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle the data for training\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training steps per epoch: 1519\n",
      "[INFO] Testing steps per epoch: 515\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps per epoch\n",
    "trainSteps = len(train_loader)\n",
    "testSteps = len(test_loader)\n",
    "\n",
    "print(f\"[INFO] Training steps per epoch: {trainSteps}\")\n",
    "print(f\"[INFO] Testing steps per epoch: {testSteps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleaned = pd.read_csv(new_csv_path)\n",
    "#metadata = df_cleaned.to_dict(orient=\"records\")\n",
    "\n",
    "#targets = df_cleaned['target'].tolist()  # Replace 'target_column_name' with actual column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train_dataset' is an instance of 'CustomMelanomaDataset'\n",
    "#train_dataset = CustomMelanomaDataset(IMAGE_DIR, metadata, targets)\n",
    "\n",
    "# Test preprocessing function separately\n",
    "#sample_metadata = {'sex': 'male', 'age_approx': 25, 'anatom_site_general_challenge': 'upper extremity'}\n",
    "#processed_metadata = train_dataset.preprocess_metadata(sample_metadata)\n",
    "#print(f\"Processed metadata: {processed_metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Initialize Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hafeez/opt/anaconda3/envs/env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hafeez/opt/anaconda3/envs/env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the model, where we pass the number of metadata features (3 in this case)\n",
    "num_metadata_features = 3  # Number of metadata features: sex, age, and site\n",
    "model = CustomResNetModel(num_metadata_features).to(DEVICE)\n",
    "\n",
    "# Loss function and optimizer\n",
    "lossFunc = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomResNetModel(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (metadata_fc): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (combined_fc): Sequential(\n",
      "    (0): Linear(in_features=544, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Training History Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store training history\n",
    "H = {\n",
    "    \"train_loss\": [], \n",
    "    \"train_acc\": [], \n",
    "    \"train_precision\": [], \n",
    "    \"train_recall\": [], \n",
    "    \"train_f1\": [], \n",
    "    \"train_roc_auc\": [],\n",
    "    \"test_roc_auc\": [],  # Only include ROC AUC for test set\n",
    "    \"test_precision\": [],\"test_recall\": [],\"test_average_precision\": [],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Loop Implementation with Metric Tracking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define metric calculation functions\n",
    "def accuracy_score(preds, labels):\n",
    "    preds = torch.sigmoid(preds).round()  # No need for conversion, assume preds is already a tensor\n",
    "    correct = (preds.squeeze() == labels).sum().item()\n",
    "    return correct / labels.size(0)\n",
    "\n",
    "def precision_score(preds, labels):\n",
    "    preds = torch.sigmoid(preds).round()  # Ensure preds is tensor\n",
    "    return skl_precision_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy(), zero_division=0)\n",
    "\n",
    "def recall_score(preds, labels):\n",
    "    preds = torch.sigmoid(preds).round()  # Ensure preds is tensor\n",
    "    return skl_recall_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy(), zero_division=0)\n",
    "\n",
    "def f1_score(preds, labels):\n",
    "    preds = torch.sigmoid(preds).round()  # Ensure preds is tensor\n",
    "    return skl_f1_score(labels.cpu().detach().numpy(), preds.cpu().detach().numpy(), zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1519 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/hafeez/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/hafeez/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CustomMelanomaDataset' on <module '__main__' (built-in)>\n",
      "Training:   0%|          | 0/1519 [01:02<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, metadata, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     16\u001b[0m         images, metadata, targets \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), metadata\u001b[38;5;241m.\u001b[39mto(DEVICE), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    startTime = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Print epoch information\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS} running...\")\n",
    "        \n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "\n",
    "        # Training\n",
    "        with tqdm(total=len(train_loader), desc='Training', unit='batch') as pbar:\n",
    "            for images, metadata, targets in train_loader:\n",
    "                images, metadata, targets = images.to(DEVICE), metadata.to(DEVICE), targets.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, metadata)\n",
    "                loss = lossFunc(outputs.squeeze(), targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_train_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Collect predictions and targets\n",
    "                all_train_preds.extend(outputs.detach())  # Keep as tensors\n",
    "                all_train_targets.extend(targets.detach()) # Keep as tensors\n",
    "                \n",
    "                pbar.update(1)  # Update progress bar\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Convert collected tensors to a stacked tensor (needed for metric functions)\n",
    "        all_train_preds = torch.stack(all_train_preds)  # Convert list of tensors to tensor\n",
    "        all_train_targets = torch.stack(all_train_targets)  # Convert list of tensors to tensor\n",
    "        \n",
    "        # Metrics calculation for training\n",
    "        train_acc = accuracy_score(all_train_preds, all_train_targets)\n",
    "        train_precision = precision_score(all_train_preds, all_train_targets)\n",
    "        train_recall = recall_score(all_train_preds, all_train_targets)\n",
    "        train_f1 = f1_score(all_train_preds, all_train_targets)\n",
    "        train_roc_auc = roc_auc_score(all_train_targets.cpu().detach().numpy(), torch.sigmoid(all_train_preds).cpu().detach().numpy())  # Compute ROC AUC\n",
    "\n",
    "        model.eval()\n",
    "        all_test_preds = []\n",
    "        all_test_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Testing (without Y targets)\n",
    "            with tqdm(total=len(test_loader), desc='Testing', unit='batch') as pbar:\n",
    "                for images, metadata, _ in test_loader:\n",
    "                    images, metadata = images.to(DEVICE), metadata.to(DEVICE)\n",
    "                    \n",
    "                    outputs = model(images, metadata)\n",
    "                    preds = torch.sigmoid(outputs).squeeze().cpu().detach().numpy()\n",
    "                    \n",
    "                    # Collect predictions (no targets available)\n",
    "                    all_test_preds.extend(preds)\n",
    "                    all_test_probs.extend(preds)\n",
    "                    \n",
    "                    pbar.update(1)  # Update progress bar\n",
    "\n",
    "        # Metrics calculation for test set (no targets)\n",
    "        # Compute ROC AUC and Precision-Recall without true Y labels (unsupervised)\n",
    "        test_roc_auc = roc_auc_score(np.zeros(len(all_test_probs)), all_test_probs)  # Assuming no ground truth, using zeros\n",
    "        \n",
    "        # Compute Precision-Recall Curve and Average Precision Score (without true labels)\n",
    "        precision, recall, _ = precision_recall_curve(np.zeros(len(all_test_probs)), all_test_probs)\n",
    "        average_precision = average_precision_score(np.zeros(len(all_test_probs)), all_test_probs)\n",
    "\n",
    "        # Store metrics in history\n",
    "        H[\"train_loss\"].append(avg_train_loss)\n",
    "        H[\"train_acc\"].append(train_acc)\n",
    "        H[\"train_precision\"].append(train_precision)\n",
    "        H[\"train_recall\"].append(train_recall)\n",
    "        H[\"train_f1\"].append(train_f1)\n",
    "        H[\"train_roc_auc\"].append(train_roc_auc)\n",
    "        H[\"test_roc_auc\"].append(test_roc_auc)\n",
    "        H[\"test_precision\"].append(precision)\n",
    "        H[\"test_recall\"].append(recall)\n",
    "        H[\"test_average_precision\"].append(average_precision)\n",
    "        \n",
    "        # Print the results for this epoch\n",
    "        print(f\"[INFO] EPOCH: {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        print(f\"Train loss: {avg_train_loss:.6f}\")\n",
    "        print(f\"Train Accuracy: {train_acc:.6f}\")\n",
    "        print(f\"Train Precision: {train_precision:.6f}\")\n",
    "        print(f\"Train Recall: {train_recall:.6f}\")\n",
    "        print(f\"Train F1 Score: {train_f1:.6f}\")\n",
    "        print(f\"Train ROC AUC: {train_roc_auc:.6f}\")\n",
    "        print(f\"Test ROC AUC: {test_roc_auc:.6f}\")\n",
    "        print(f\"Test Average Precision Score: {average_precision:.6f}\")\n",
    "        print()\n",
    "\n",
    "    endTime = time.time()\n",
    "    print(f\"[INFO] Total time taken to train the model: {np.round(endTime - startTime)} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metrics(H):\n",
    "    epochs = range(1, len(H[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.plot(epochs, H[\"train_loss\"], 'b', label='Train Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(3, 3, 2)\n",
    "    plt.plot(epochs, H[\"train_acc\"], 'b', label='Train Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Precision\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.plot(epochs, H[\"train_precision\"], 'b', label='Train Precision')\n",
    "    plt.title('Training Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Recall\n",
    "    plt.subplot(3, 3, 4)\n",
    "    plt.plot(epochs, H[\"train_recall\"], 'b', label='Train Recall')\n",
    "    plt.title('Training Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot F1 Score\n",
    "    plt.subplot(3, 3, 5)\n",
    "    plt.plot(epochs, H[\"train_f1\"], 'b', label='Train F1 Score')\n",
    "    plt.title('Training F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot ROC AUC Score\n",
    "    plt.subplot(3, 3, 6)\n",
    "    plt.plot(epochs, H[\"train_roc_auc\"], 'b', label='Train ROC AUC')\n",
    "    plt.plot(epochs, H[\"test_roc_auc\"], 'r', label='Test ROC AUC')\n",
    "    plt.title('ROC AUC Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Precision-Recall Curve\n",
    "    plt.subplot(3, 3, 7)\n",
    "    for i, (precision, recall) in enumerate(zip(H[\"test_precision\"], H[\"test_recall\"])):\n",
    "        plt.plot(recall, precision, label=f'Epoch {i+1}')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Average Precision Score\n",
    "    plt.subplot(3, 3, 8)\n",
    "    plt.plot(epochs, H[\"test_average_precision\"], 'b', label='Test Average Precision')\n",
    "    plt.title('Average Precision Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the metrics\n",
    "plot_metrics(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot ROC AUC for testing\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(H[\"test_roc_auc\"], label='Test ROC AUC', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('Testing ROC AUC')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
