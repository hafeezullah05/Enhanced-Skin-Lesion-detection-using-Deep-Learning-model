{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from natsort import natsorted # type: ignore\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import precision_score as skl_precision_score\n",
    "from sklearn.metrics import recall_score as skl_recall_score\n",
    "from sklearn.metrics import f1_score as skl_f1_score\n",
    "from sklearn.metrics import accuracy_score as skl_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import the code from all .py files\n",
    "\n",
    "from custom_dataset import CustomMelanomaDataset  # Import the custom dataset\n",
    "from resnet_model import ResNetModel\n",
    "from preprocessing_csv import PreprocessingCSV\n",
    "from train_test_loop import train_and_test\n",
    "from lesion_predictions import LesionPredictions\n",
    "from FocalLoss import FocalLoss\n",
    "#from malignant_augmentation import TrainMalignantAugmentor, TestMalignantAugmentor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import torch.profiler\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "#from torch.profiler import profile, ProfilerActivity\n",
    "#import tkinter as tk\n",
    "#from tkinter import filedialog, messagebox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
      "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
      "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
      "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
      "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
      "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
      "\n",
      "  diagnosis benign_malignant  target  \n",
      "0   unknown           benign     0.0  \n",
      "1   unknown           benign     0.0  \n",
      "2     nevus           benign     0.0  \n",
      "3   unknown           benign     0.0  \n",
      "4   unknown           benign     0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33126 entries, 0 to 33125\n",
      "Data columns (total 8 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   image_name                     33126 non-null  object \n",
      " 1   patient_id                     33126 non-null  object \n",
      " 2   sex                            33061 non-null  object \n",
      " 3   age_approx                     33058 non-null  float64\n",
      " 4   anatom_site_general_challenge  32599 non-null  object \n",
      " 5   diagnosis                      33126 non-null  object \n",
      " 6   benign_malignant               33126 non-null  object \n",
      " 7   target                         33125 non-null  float64\n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 2.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the dynamic root path\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Display the structure of the dataset\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the universal path handling logic\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "SPLIT_CSV_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv')\n",
    "\n",
    "# Universal Path Setup for Images\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'Train_JPEG', 'JPEG')\n",
    "\n",
    "# Paths for Train/Test CSVs\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_split.csv')\n",
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "preprocess_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv')\n",
    "\n",
    "# to overcome class imbalance\n",
    "TRAIN_CSV_PASS1 = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_pass1.csv')\n",
    "TRAIN_CSV_PASS2 = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_pass2.csv')\n",
    "\n",
    "# save model after training/testing\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez',  'Thesis_Code/Enhanced-Skin-Lesion-detection-using-Deep-Learning-model/results', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Unique values in 'anatom_site_general_challenge': ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'nan'\n",
      " 'lower extremityi wrote to him' 'palms/soles' 'oral/genital']\n",
      "\n",
      " \n",
      " Rows with variations of 'unknown':\n",
      " Empty DataFrame\n",
      "Columns: [image_name, patient_id, sex, age_approx, anatom_site_general_challenge, diagnosis, benign_malignant, target]\n",
      "Index: []\n",
      "\n",
      " \n",
      " Unique values in 'anatom_site_general_challenge' after cleanup: ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'unknown'\n",
      " 'palms/soles' 'oral/genital']\n",
      "\n",
      " \n",
      " NaN values in each column:\n",
      " image_name                        0\n",
      "patient_id                        0\n",
      "sex                              65\n",
      "age_approx                       68\n",
      "anatom_site_general_challenge     0\n",
      "diagnosis                         0\n",
      "benign_malignant                  0\n",
      "target                            1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      " Infinity values in each column:\n",
      " image_name                       0\n",
      "patient_id                       0\n",
      "sex                              0\n",
      "age_approx                       0\n",
      "anatom_site_general_challenge    0\n",
      "diagnosis                        0\n",
      "benign_malignant                 0\n",
      "target                           0\n",
      "dtype: int64\n",
      "\n",
      " \n",
      " Number of 'unknown' values in 'anatom_site_general_challenge': 527\n",
      "\n",
      " \n",
      "Maximum values in each column (excluding 'diagnosis'):\n",
      " age_approx    90.0\n",
      "target         1.0\n",
      "dtype: float64\n",
      "\n",
      " \n",
      " Preprocessed CSV saved to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv.\n",
      "\n",
      "Train and test CSV files have been written to 'split_csv'.\n",
      "\n",
      " \n",
      " NaN values in each column:\n",
      " image_name                       0\n",
      "patient_id                       0\n",
      "sex                              0\n",
      "age_approx                       0\n",
      "anatom_site_general_challenge    0\n",
      "benign_malignant                 0\n",
      "target                           0\n",
      "dtype: int64\n",
      "\n",
      " \n",
      " Infinity values in each column:\n",
      " image_name                       0\n",
      "patient_id                       0\n",
      "sex                              0\n",
      "age_approx                       0\n",
      "anatom_site_general_challenge    0\n",
      "benign_malignant                 0\n",
      "target                           0\n",
      "dtype: int64\n",
      "\n",
      " \n",
      " Number of 'unknown' values in 'anatom_site_general_challenge': 0\n",
      "\n",
      " \n",
      " Unique values in 'anatom_site_general_challenge': ['head/neck' 'upper extremity' 'lower extremity' 'torso' 'palms/soles'\n",
      " 'oral/genital']\n",
      "\n",
      " \n",
      " Maximum values in each column (excluding 'diagnosis'):\n",
      " age_approx    90.0\n",
      "target         1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step : Create an instance of PreprocessingCSV\n",
    "preprocessor = PreprocessingCSV(CSV_PATH, BASE_DIR)\n",
    "\n",
    "# Step : Execute the preprocessing steps\n",
    "preprocessor.analyze_raw_data()\n",
    "preprocessor.check_for_anomalies()\n",
    "preprocessor.clean_data()\n",
    "preprocessor.save_clean_data()\n",
    "preprocessor.split_by_patient_id()\n",
    "preprocessor.verify_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio of dataset. Analysis of Benign/Malignant in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets in training dataset:\n",
      "Benign (0.0): 23861\n",
      "Malignant (1.0): 439\n",
      "\n",
      "Targets in test dataset:\n",
      "Benign (0.0): 8094\n",
      "Malignant (1.0): 136\n"
     ]
    }
   ],
   "source": [
    "def count_targets(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    benign_count = df[df['target'] == 0].shape[0]\n",
    "    malignant_count = df[df['target'] == 1].shape[0]\n",
    "    return benign_count, malignant_count\n",
    "\n",
    "# Count targets in the training dataset\n",
    "train_benign_count, train_malignant_count = count_targets(TRAIN_CSV_PATH)\n",
    "print(f\"Targets in training dataset:\\nBenign (0.0): {train_benign_count}\\nMalignant (1.0): {train_malignant_count}\")\n",
    "\n",
    "# Count targets in the test dataset\n",
    "test_benign_count, test_malignant_count = count_targets(TEST_CSV_PATH)\n",
    "print(f\"\\nTargets in test dataset:\\nBenign (0.0): {test_benign_count}\\nMalignant (1.0): {test_malignant_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating offline Augmentations for traing and test dataset to overcome class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here must run malignant_augmentation.py seperately.\n",
    "\n",
    "# Must run this .py file to proceed further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainMalignantAugmentor:\n",
    "    def __init__(self, train_aug_csv, image_dir, augmentations_per_image=15):\n",
    "        self.train_aug_csv = train_aug_csv\n",
    "        self.image_dir = image_dir\n",
    "        self.augmentations_per_image = augmentations_per_image\n",
    "        self.data = pd.read_csv(self.train_aug_csv)\n",
    "        self.original_data_length = len(self.data)\n",
    "        \n",
    "        # Define the augmentation transformations\n",
    "        self.augment_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def augment_image(self, image_path, image_name_prefix):\n",
    "        \"\"\"\n",
    "        Perform augmentations on a given image and return augmented images with new metadata entries.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original_image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "        augmented_entries = []\n",
    "\n",
    "        for i in range(1, self.augmentations_per_image + 1):\n",
    "            augmented_image = self.augment_transform(original_image)\n",
    "            augmented_image_name = f\"{image_name_prefix}_aug_{i}.JPG\"\n",
    "            augmented_image_path = os.path.join(self.image_dir, augmented_image_name)\n",
    "\n",
    "            # Save augmented image (specify format explicitly as 'JPEG')\n",
    "            transforms.ToPILImage()(augmented_image).save(augmented_image_path, format='JPEG')\n",
    "\n",
    "            # Create metadata entry for the augmented image\n",
    "            augmented_entries.append(augmented_image_name)\n",
    "\n",
    "        return augmented_entries\n",
    "\n",
    "    def perform_augmentation(self):\n",
    "        augmented_metadata = []\n",
    "\n",
    "        with tqdm(total=self.original_data_length, desc=\"Performing Image Augmentation\", unit=\"image\") as pbar:\n",
    "            for idx in range(self.original_data_length):\n",
    "                row = self.data.iloc[idx]\n",
    "                image_name = row['image_name']\n",
    "                benign_malignant = row['benign_malignant']\n",
    "\n",
    "                # Skip already augmented images\n",
    "                if \"_aug_\" in image_name:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Only augment malignant cases\n",
    "                if benign_malignant == 'malignant':\n",
    "                    image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "                    # Check if the file exists\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Image not found: {image_path}\")\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    augmented_image_names = self.augment_image(image_path, image_name)\n",
    "\n",
    "                    # Append metadata for augmented images\n",
    "                    for aug_image_name in augmented_image_names:\n",
    "                        augmented_row = row.copy()\n",
    "                        augmented_row['image_name'] = aug_image_name\n",
    "                        augmented_metadata.append(augmented_row)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Append augmented metadata to the original DataFrame\n",
    "        augmented_metadata_df = pd.DataFrame(augmented_metadata)\n",
    "        self.data = pd.concat([self.data, augmented_metadata_df], ignore_index=True)\n",
    "\n",
    "        print(\"Image augmentations for training dataset is complete.\")\n",
    "\n",
    "    def save_updated_metadata(self):\n",
    "        \"\"\"Save the updated metadata to the original CSV file.\"\"\"\n",
    "        self.data.to_csv(self.train_aug_csv, index=False)\n",
    "        print(f\"Updated metadata saved to {self.train_aug_csv}\")\n",
    "        print(\"Process finished successfully.\")\n",
    "\n",
    "\n",
    "class TestMalignantAugmentor:\n",
    "    def __init__(self, test_aug_csv, image_dir, augmentations_per_image=10):\n",
    "        self.test_aug_csv = test_aug_csv\n",
    "        self.image_dir = image_dir\n",
    "        self.augmentations_per_image = augmentations_per_image\n",
    "        self.data = pd.read_csv(self.test_aug_csv)\n",
    "        self.original_data_length = len(self.data)\n",
    "        \n",
    "        # Define the augmentation transformations\n",
    "        self.augment_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def augment_image(self, image_path, image_name_prefix):\n",
    "        \"\"\"\n",
    "        Perform augmentations on a given image and return augmented images with new metadata entries.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original_image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "        augmented_entries = []\n",
    "\n",
    "        for i in range(1, self.augmentations_per_image + 1):\n",
    "            augmented_image = self.augment_transform(original_image)\n",
    "            augmented_image_name = f\"{image_name_prefix}_aug_{i}.JPG\"\n",
    "            augmented_image_path = os.path.join(self.image_dir, augmented_image_name)\n",
    "\n",
    "            # Save augmented image (specify format explicitly as 'JPEG')\n",
    "            transforms.ToPILImage()(augmented_image).save(augmented_image_path, format='JPEG')\n",
    "\n",
    "            # Create metadata entry for the augmented image\n",
    "            augmented_entries.append(augmented_image_name)\n",
    "\n",
    "        return augmented_entries\n",
    "\n",
    "    def perform_augmentation(self):\n",
    "        augmented_metadata = []\n",
    "\n",
    "        with tqdm(total=self.original_data_length, desc=\"Performing Image Augmentation (Test)\", unit=\"image\") as pbar:\n",
    "            for idx in range(self.original_data_length):\n",
    "                row = self.data.iloc[idx]\n",
    "                image_name = row['image_name']\n",
    "                benign_malignant = row['benign_malignant']\n",
    "\n",
    "                # Skip already augmented images\n",
    "                if \"_aug_\" in image_name:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Only augment malignant cases\n",
    "                if benign_malignant == 'malignant':\n",
    "                    image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "                    # Check if the file exists\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Image not found: {image_path}\")\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    augmented_image_names = self.augment_image(image_path, image_name)\n",
    "\n",
    "                    # Append metadata for augmented images\n",
    "                    for aug_image_name in augmented_image_names:\n",
    "                        augmented_row = row.copy()\n",
    "                        augmented_row['image_name'] = aug_image_name\n",
    "                        augmented_metadata.append(augmented_row)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Append augmented metadata to the original DataFrame\n",
    "        augmented_metadata_df = pd.DataFrame(augmented_metadata)\n",
    "        self.data = pd.concat([self.data, augmented_metadata_df], ignore_index=True)\n",
    "\n",
    "        print(\"Image augmentations for test dataset is complete.\")\n",
    "\n",
    "    def save_updated_metadata(self):\n",
    "        \"\"\"Save the updated metadata to the original CSV file.\"\"\"\n",
    "        self.data.to_csv(self.test_aug_csv, index=False)\n",
    "        print(f\"Updated metadata saved to {self.test_aug_csv}\")\n",
    "        print(\"Process finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Image Augmentation: 100%|██████████| 24300/24300 [1:44:32<00:00,  3.87image/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image augmentations for training dataset is complete.\n",
      "Updated metadata saved to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/split_csv/train_split.csv\n",
      "Process finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Image Augmentation (Test): 100%|██████████| 8230/8230 [24:30<00:00,  5.60image/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image augmentations for test dataset is complete.\n",
      "Updated metadata saved to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/split_csv/test_split.csv\n",
      "Process finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "    IMAGE_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'Train_JPEG', 'JPEG')\n",
    "    TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_split.csv')\n",
    "    TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "\n",
    "    # Initialize the augmentor for training data\n",
    "    train_augmentor = TrainMalignantAugmentor(\n",
    "        train_aug_csv=TRAIN_CSV_PATH,\n",
    "        image_dir=IMAGE_DIR\n",
    "    )\n",
    "\n",
    "    # Perform augmentation and save the updated metadata for training\n",
    "    train_augmentor.perform_augmentation()\n",
    "    train_augmentor.save_updated_metadata()\n",
    "\n",
    "    # Initialize the augmentor for test data\n",
    "    test_augmentor = TestMalignantAugmentor(\n",
    "        test_aug_csv=TEST_CSV_PATH,\n",
    "        image_dir=IMAGE_DIR\n",
    "    )\n",
    "\n",
    "    # Perform augmentation and save the updated metadata for testing\n",
    "    test_augmentor.perform_augmentation()\n",
    "    test_augmentor.save_updated_metadata()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio after Augmemntation of target Benign/Malignants in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets in training dataset:\n",
      "Benign (0.0): 23861\n",
      "Malignant (1.0): 7024\n",
      "\n",
      "Targets in test dataset:\n",
      "Benign (0.0): 8094\n",
      "Malignant (1.0): 1496\n"
     ]
    }
   ],
   "source": [
    "def count_targets(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    benign_count = df[df['target'] == 0].shape[0]\n",
    "    malignant_count = df[df['target'] == 1].shape[0]\n",
    "    return benign_count, malignant_count\n",
    "\n",
    "# Count targets in the training dataset\n",
    "train_benign_count, train_malignant_count = count_targets(TRAIN_CSV_PATH)\n",
    "print(f\"Targets in training dataset:\\nBenign (0.0): {train_benign_count}\\nMalignant (1.0): {train_malignant_count}\")\n",
    "\n",
    "# Count targets in the test dataset\n",
    "test_benign_count, test_malignant_count = count_targets(TEST_CSV_PATH)\n",
    "print(f\"\\nTargets in test dataset:\\nBenign (0.0): {test_benign_count}\\nMalignant (1.0): {test_malignant_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Benign into 2 phase trainnig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24k benign trained in 2 stages, 2 * (12k benign + 7K malignant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'benign_malignant' column: ['benign' 'malignant']\n",
      "Total benign cases: 23861\n",
      "Total malignant cases: 7024\n",
      "Pass 1 saved to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/split_csv/train_pass1.csv\n",
      "Pass 2 saved to /Users/hafeez/Desktop/Thesis_Hafeez/Dataset/split_csv/train_pass2.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# Verify the labels in the dataset\n",
    "print(\"Unique values in 'benign_malignant' column:\", data['benign_malignant'].unique())\n",
    "\n",
    "# Group by patient_id\n",
    "grouped = data.groupby('patient_id')\n",
    "\n",
    "# Separate benign and malignant cases\n",
    "benign_data = data[data['benign_malignant'] == 'benign']\n",
    "malignant_data = data[data['benign_malignant'] == 'malignant']\n",
    "\n",
    "# Check the number of benign and malignant samples before proceeding\n",
    "print(f\"Total benign cases: {len(benign_data)}\")\n",
    "print(f\"Total malignant cases: {len(malignant_data)}\")\n",
    "\n",
    "if len(malignant_data) < 7024:\n",
    "    print(\"Warning: Malignant data count is less than expected. Please check the data source or filtering logic.\")\n",
    "\n",
    "# Shuffle benign patient groups and split approximately in half\n",
    "benign_patient_ids = list(benign_data['patient_id'].unique())\n",
    "random.shuffle(benign_patient_ids)\n",
    "split_index = len(benign_patient_ids) // 2\n",
    "\n",
    "# Split benign data into two halves based on patient IDs\n",
    "benign_pass1 = benign_data[benign_data['patient_id'].isin(benign_patient_ids[:split_index])]\n",
    "benign_pass2 = benign_data[benign_data['patient_id'].isin(benign_patient_ids[split_index:])]\n",
    "\n",
    "# Add all malignant cases to both pass1 and pass2\n",
    "train_pass1 = pd.concat([benign_pass1, malignant_data], ignore_index=True)\n",
    "train_pass2 = pd.concat([benign_pass2, malignant_data], ignore_index=True)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(os.path.dirname(TRAIN_CSV_PASS1)):\n",
    "    os.makedirs(os.path.dirname(TRAIN_CSV_PASS1))\n",
    "\n",
    "# Save the passes to their respective CSV files\n",
    "train_pass1.to_csv(TRAIN_CSV_PASS1, index=False)\n",
    "train_pass2.to_csv(TRAIN_CSV_PASS2, index=False)\n",
    "\n",
    "print(f\"Pass 1 saved to {TRAIN_CSV_PASS1}\")\n",
    "print(f\"Pass 2 saved to {TRAIN_CSV_PASS2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass 1 - Benign: 11943, Malignant: 7024\n",
      "Pass 2 - Benign: 11918, Malignant: 7024\n"
     ]
    }
   ],
   "source": [
    "# Count and print the number of benign and malignant cases in each pass\n",
    "def count_cases(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    benign_count = len(data[data['benign_malignant'] == 'benign'])\n",
    "    malignant_count = len(data[data['benign_malignant'] == 'malignant'])\n",
    "    return benign_count, malignant_count\n",
    "\n",
    "benign_count_pass1, malignant_count_pass1 = count_cases(TRAIN_CSV_PASS1)\n",
    "benign_count_pass2, malignant_count_pass2 = count_cases(TRAIN_CSV_PASS2)\n",
    "\n",
    "print(f\"\\nPass 1 - Benign: {benign_count_pass1}, Malignant: {malignant_count_pass1}\")\n",
    "print(f\"Pass 2 - Benign: {benign_count_pass2}, Malignant: {malignant_count_pass2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total benign cases: 23861\n",
      "Benign cases in Pass 1: 11943\n",
      "Benign cases in Pass 2: 11918\n",
      "Total malignant cases: 7024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total benign cases: {len(benign_data)}\")\n",
    "print(f\"Benign cases in Pass 1: {len(benign_pass1)}\")\n",
    "print(f\"Benign cases in Pass 2: {len(benign_pass2)}\")\n",
    "print(f\"Total malignant cases: {len(malignant_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a consistent size\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(),  # Random vertical flip\n",
    "    transforms.RandomRotation(30),  # Random rotation for variety\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Color jitter for diversity\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Testing Transformations\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a consistent size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets for Pass 1 and Pass 2\n",
    "train_pass1_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PASS1,  # CSV for Pass 1 with 12k benign + all malignant\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "train_pass2_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PASS2,  # CSV for Pass 2 with the remaining 12k benign + all malignant\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "test_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TEST_CSV_PATH,  # Test CSV with augmented malignant and original benign samples\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=test_transforms,\n",
    "    is_test=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "# Create datasets\n",
    "\"\"\"\n",
    "train_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PATH,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False  # Indicates that this dataset is for training\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TEST_CSV_PATH,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=test_transforms,\n",
    "    is_test=True  # Indicates that this dataset is for testing\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "lr = 1e-5\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "num_workers=4\n",
    "\n",
    "# Determine if CUDA is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "print(f\"[INFO] Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloaders, lossFunc, Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights for Pass 1 dataset\n",
    "malignant_count_pass1 = len(train_pass1_dataset.metadata[train_pass1_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count_pass1 = len(train_pass1_dataset.metadata[train_pass1_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count_pass1 = len(train_pass1_dataset)\n",
    "\n",
    "# Set weights: higher for minority class (for Pass 1)\n",
    "weights_pass1 = [\n",
    "    benign_count_pass1 / total_count_pass1 if label == 0 else malignant_count_pass1 / total_count_pass1\n",
    "    for label in train_pass1_dataset.metadata['target']\n",
    "]\n",
    "sampler_pass1 = WeightedRandomSampler(weights_pass1, len(weights_pass1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights for Pass 2 dataset\n",
    "malignant_count_pass2 = len(train_pass2_dataset.metadata[train_pass2_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count_pass2 = len(train_pass2_dataset.metadata[train_pass2_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count_pass2 = len(train_pass2_dataset)\n",
    "\n",
    "# Set weights: higher for minority class (for Pass 2)\n",
    "weights_pass2 = [\n",
    "    benign_count_pass2 / total_count_pass2 if label == 0 else malignant_count_pass2 / total_count_pass2\n",
    "    for label in train_pass2_dataset.metadata['target']\n",
    "]\n",
    "sampler_pass2 = WeightedRandomSampler(weights_pass2, len(weights_pass2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\n",
    "# Compute weights for each class\n",
    "malignant_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count = len(train_dataset)\n",
    "\n",
    "# Set weights: higher for minority class\n",
    "weights = [benign_count / total_count if label == 0 else malignant_count / total_count for label in train_dataset.metadata['target']]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "When using a sampler, you should remove the shuffle argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataLoaders for Pass 1, Pass 2, and Test dataset\n",
    "# Create DataLoaders with weighted sampler for Pass 1 and Pass 2\n",
    "train_pass1_loader = DataLoader(\n",
    "    dataset=train_pass1_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_pass1,  # Use weighted random sampler for class balancing\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "train_pass2_loader = DataLoader(\n",
    "    dataset=train_pass2_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_pass2,  # Use weighted random sampler for class balancing\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    #sampler=sampler, # Set weights: higher for minority class\n",
    "    shuffle=True,  # Shuffle the data for training # When using a sampler, \n",
    "    # I should remove the shuffle argument.\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training steps per epoch: 1186\n",
      "[INFO] Training steps per epoch: 1184\n",
      "[INFO] Testing steps per epoch: 600\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps per epoch\n",
    "trainSteps1 = len(train_pass1_loader)\n",
    "trainSteps2 = len(train_pass2_loader)\n",
    "\n",
    "testSteps = len(test_loader)\n",
    "\n",
    "print(f\"[INFO] Training steps per epoch: {trainSteps1}\")\n",
    "print(f\"[INFO] Training steps per epoch: {trainSteps2}\")\n",
    "print(f\"[INFO] Testing steps per epoch: {testSteps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Initialize Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' # old approach\n",
    "# Calculate class weights for weighted cross entropy\n",
    "benign_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "\n",
    "malignant_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "\n",
    "# Adjust weight for handling class imbalance\n",
    "pos_weight = torch.tensor([benign_count / malignant_count], dtype=torch.float).to(DEVICE)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, where we pass the number of metadata features (3 in this case)\n",
    "num_metadata_features = 3  # Number of metadata features: sex, age, and site\n",
    "model = ResNetModel(num_metadata_features).to(DEVICE)\n",
    "\n",
    "# Loss function and optimizer #old lossFunc\n",
    "#lossFunc = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "lossFunc = FocalLoss(alpha=3, gamma=2)  # Adjust alpha to give more weight to malignant class\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#old scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Training History Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Loop Implementation with Metric Tracking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define the parameters for the training and test loop\n",
    "H = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Pass 1 Dataset\n",
      "\n",
      "Epoch 1/10 running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 334/1186 [1:14:31<4:29:14, 18.96s/batch]"
     ]
    }
   ],
   "source": [
    "# Train on Pass 1 Dataset\n",
    "print(\"Starting training with Pass 1 Dataset\")\n",
    "H_pass1 = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_pass1_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n",
    "# After training, we can use H for further analysis or plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Pass 2 Dataset\n",
    "print(\"Starting training with Pass 2 Dataset\")\n",
    "H_pass2 = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_pass2_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training loop ends, save the model\n",
    "# Ensure the directory exists, if not, create it\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "# Define the model filename with the .pth extension\n",
    "model_filename = \"melanoma_trained_model.pth\"\n",
    "\n",
    "# Full path to save the model\n",
    "model_save_path = os.path.join(MODEL_PATH, model_filename)\n",
    "\n",
    "# Save the model, replacing if it already exists\n",
    "if os.path.exists(model_save_path):\n",
    "    os.remove(model_save_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics for combined Pass 1 and Pass 2\n",
    "def plot_combined_metrics(H_pass1, H_pass2):\n",
    "    # Combine metrics from Pass 1 and Pass 2\n",
    "    combined_H = {}\n",
    "    for key in H_pass1:\n",
    "        combined_H[key] = H_pass1[key] + H_pass2[key]  # Concatenate metrics for Pass 1 and Pass 2\n",
    "    \n",
    "    # Number of epochs for combined training\n",
    "    epochs = range(1, len(combined_H[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 20))\n",
    "\n",
    "    # Plot Training and Test Loss\n",
    "    plt.subplot(4, 2, 1)\n",
    "    plt.plot(epochs, combined_H[\"train_loss\"], 'b', label='Train Loss')\n",
    "    plt.plot(epochs, combined_H[\"test_loss\"], 'r', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Accuracy\n",
    "    plt.subplot(4, 2, 2)\n",
    "    plt.plot(epochs, combined_H[\"train_acc\"], 'b', label='Train Accuracy')\n",
    "    plt.plot(epochs, combined_H[\"test_acc\"], 'r', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Precision\n",
    "    plt.subplot(4, 2, 3)\n",
    "    plt.plot(epochs, combined_H[\"train_precision\"], 'b', label='Train Precision')\n",
    "    plt.plot(epochs, combined_H[\"test_precision\"], 'r', label='Test Precision')\n",
    "    plt.title('Training and Test Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Recall\n",
    "    plt.subplot(4, 2, 4)\n",
    "    plt.plot(epochs, combined_H[\"train_recall\"], 'b', label='Train Recall')\n",
    "    plt.plot(epochs, combined_H[\"test_recall\"], 'r', label='Test Recall')\n",
    "    plt.title('Training and Test Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test F1 Score\n",
    "    plt.subplot(4, 2, 5)\n",
    "    plt.plot(epochs, combined_H[\"train_f1\"], 'b', label='Train F1 Score')\n",
    "    plt.plot(epochs, combined_H[\"test_f1\"], 'r', label='Test F1 Score')\n",
    "    plt.title('Training and Test F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test ROC AUC\n",
    "    plt.subplot(4, 2, 6)\n",
    "    plt.plot(epochs, combined_H[\"train_roc_auc\"], 'b', label='Train ROC AUC')\n",
    "    plt.plot(epochs, combined_H[\"test_roc_auc\"], 'r', label='Test ROC AUC')\n",
    "    plt.title('Training and Test ROC AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Precision-Recall Curve for each epoch (from Pass 1 and Pass 2)\n",
    "    plt.subplot(4, 2, 7)\n",
    "    for i, (precision, recall) in enumerate(combined_H[\"test_precision_recall_curve\"]):\n",
    "        plt.plot(recall, precision, label=f'Epoch {i+1}')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Average Precision Score\n",
    "    plt.subplot(4, 2, 8)\n",
    "    plt.plot(epochs, combined_H[\"test_average_precision\"], 'b', label='Test Average Precision')\n",
    "    plt.title('Average Precision Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the combined metrics for Pass 1 and Pass 2\n",
    "plot_combined_metrics(H_pass1, H_pass2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the plot to the specified path\n",
    "plot_filename = os.path.join(MODEL_PATH, \"training_metrics_plot.png\")\n",
    "\n",
    "# Use a higher DPI for better quality\n",
    "plt.savefig(plot_filename, format='png', dpi=300)  # Save the plot with higher DPI for clarity\n",
    "print(f\"Plot saved to {plot_filename}\")\n",
    "    \n",
    "plt.close()  # Ensure the plot is cleared after saving to avoid showing it blank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally prediction on lesions (Benign/Malignant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "test_metadata_df = LesionPredictions.load_metadata(TEST_CSV_PATH)\n",
    "\n",
    "# Make sure to define 'model', 'test_loader', and 'DEVICE'\n",
    "visualizer = LesionPredictions(model, test_loader, DEVICE)\n",
    "visualizer.inference_prediction()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
