{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_test_loop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_and_test\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minteractive_visual_comparison\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_metadata, interactive_visual_comparison\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFocalLoss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FocalLoss\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#import torch.profiler\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#from torch.utils.tensorboard import SummaryWriter\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#from tensorboardX import SummaryWriter\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#from torch.profiler import profile, ProfilerActivity\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#import tkinter as tk\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#from tkinter import filedialog, messagebox\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Thesis_Hafeez/Thesis_Code/Enhanced-Skin-Lesion-detection-using-Deep-Learning-model/code/FocalLoss.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFocalLoss\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(FocalLoss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "#from natsort import natsorted # type: ignore\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import precision_score as skl_precision_score\n",
    "from sklearn.metrics import recall_score as skl_recall_score\n",
    "from sklearn.metrics import f1_score as skl_f1_score\n",
    "from sklearn.metrics import accuracy_score as skl_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import the code from all .py files\n",
    "\n",
    "from custom_dataset import CustomMelanomaDataset  # Import the custom dataset\n",
    "from resnet_model import ResNetModel\n",
    "from preprocessing_csv import PreprocessingCSV\n",
    "from train_test_loop import train_and_test\n",
    "from lesion_predictions import LesionPredictions\n",
    "from FocalLoss import FocalLoss\n",
    "from malignant_augmentation import TrainMalignantAugmentor, TestMalignantAugmentor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import torch.profiler\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "#from torch.profiler import profile, ProfilerActivity\n",
    "#import tkinter as tk\n",
    "#from tkinter import filedialog, messagebox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic root path\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Display the structure of the dataset\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Define the universal path handling logic\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m SPLIT_CSV_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThesis_Hafeez\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Universal Path Setup for Images\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the universal path handling logic\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "SPLIT_CSV_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv')\n",
    "\n",
    "# Universal Path Setup for Images\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'Train_JPEG', 'JPEG')\n",
    "\n",
    "# Paths for Train/Test CSVs\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_split.csv')\n",
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "preprocess_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv')\n",
    "\n",
    "# to overcome class imbalance\n",
    "TRAIN_CSV_PASS1 = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_pass1.csv')\n",
    "TRAIN_CSV_PASS2 = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_pass2.csv')\n",
    "\n",
    "# save model after training/testing\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez',  'Thesis_Code/Enhanced-Skin-Lesion-detection-using-Deep-Learning-model/results', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step : Create an instance of PreprocessingCSV\n",
    "preprocessor = PreprocessingCSV(CSV_PATH, BASE_DIR)\n",
    "\n",
    "# Step : Execute the preprocessing steps\n",
    "preprocessor.analyze_raw_data()\n",
    "preprocessor.check_for_anomalies()\n",
    "preprocessor.clean_data()\n",
    "preprocessor.save_clean_data()\n",
    "preprocessor.split_by_patient_id()\n",
    "preprocessor.verify_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio of dataset. Analysis of Benign/Malignant in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_targets(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    benign_count = df[df['target'] == 0].shape[0]\n",
    "    malignant_count = df[df['target'] == 1].shape[0]\n",
    "    return benign_count, malignant_count\n",
    "\n",
    "# Count targets in the training dataset\n",
    "train_benign_count, train_malignant_count = count_targets(TRAIN_CSV_PATH)\n",
    "print(f\"Targets in training dataset:\\nBenign (0.0): {train_benign_count}\\nMalignant (1.0): {train_malignant_count}\")\n",
    "\n",
    "# Count targets in the test dataset\n",
    "test_benign_count, test_malignant_count = count_targets(TEST_CSV_PATH)\n",
    "print(f\"\\nTargets in test dataset:\\nBenign (0.0): {test_benign_count}\\nMalignant (1.0): {test_malignant_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating offline Augmentations for traing and test dataset to overcome class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here must run malignant_augmentation.py seperately.\n",
    "\n",
    "# Must run this .py file to proceed further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio after Augmemntation of target Benign/Malignants in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_targets(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    benign_count = df[df['target'] == 0].shape[0]\n",
    "    malignant_count = df[df['target'] == 1].shape[0]\n",
    "    return benign_count, malignant_count\n",
    "\n",
    "# Count targets in the training dataset\n",
    "train_benign_count, train_malignant_count = count_targets(TRAIN_CSV_PATH)\n",
    "print(f\"Targets in training dataset:\\nBenign (0.0): {train_benign_count}\\nMalignant (1.0): {train_malignant_count}\")\n",
    "\n",
    "# Count targets in the test dataset\n",
    "test_benign_count, test_malignant_count = count_targets(TEST_CSV_PATH)\n",
    "print(f\"\\nTargets in test dataset:\\nBenign (0.0): {test_benign_count}\\nMalignant (1.0): {test_malignant_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Benign into 2 phase trainnig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24k benign trained in 2 stages, 2 * (12k benign + 7K malignant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# Group by patient_id\n",
    "grouped = data.groupby('patient_id')\n",
    "\n",
    "# Separate benign and malignant cases\n",
    "benign_groups = grouped.filter(lambda x: x.iloc[0]['benign_malignant'] == 'benign').groupby('patient_id')\n",
    "malignant_groups = grouped.filter(lambda x: x.iloc[0]['benign_malignant'] == 'malignant').groupby('patient_id')\n",
    "\n",
    "# Shuffle benign patient groups and split approximately in half\n",
    "benign_patient_ids = list(benign_groups.groups.keys())\n",
    "random.shuffle(benign_patient_ids)\n",
    "split_index = len(benign_patient_ids) // 2\n",
    "\n",
    "benign_pass1 = benign_groups.filter(lambda x: x.iloc[0]['patient_id'] in benign_patient_ids[:split_index])\n",
    "benign_pass2 = benign_groups.filter(lambda x: x.iloc[0]['patient_id'] in benign_patient_ids[split_index:])\n",
    "\n",
    "# Add all malignant cases to both pass1 and pass2\n",
    "malignant_pass = pd.concat([malignant_groups.get_group(patient_id) for patient_id in malignant_groups.groups])\n",
    "\n",
    "# Combine benign and malignant cases for pass 1 and pass 2\n",
    "train_pass1 = pd.concat([benign_pass1, malignant_pass])\n",
    "train_pass2 = pd.concat([benign_pass2, malignant_pass])\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(os.path.dirname(TRAIN_CSV_PASS1)):\n",
    "    os.makedirs(os.path.dirname(TRAIN_CSV_PASS1))\n",
    "\n",
    "# Save the passes to their respective CSV files\n",
    "train_pass1.to_csv(TRAIN_CSV_PASS1, index=False)\n",
    "train_pass2.to_csv(TRAIN_CSV_PASS2, index=False)\n",
    "\n",
    "print(f\"Pass 1 saved to {TRAIN_CSV_PASS1}\")\n",
    "print(f\"Pass 2 saved to {TRAIN_CSV_PASS2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and print the number of benign and malignant cases in each pass\n",
    "def count_cases(csv_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    benign_count = len(data[data['benign_malignant'] == 'benign'])\n",
    "    malignant_count = len(data[data['benign_malignant'] == 'malignant'])\n",
    "    return benign_count, malignant_count\n",
    "\n",
    "benign_count_pass1, malignant_count_pass1 = count_cases(TRAIN_CSV_PASS1)\n",
    "benign_count_pass2, malignant_count_pass2 = count_cases(TRAIN_CSV_PASS2)\n",
    "\n",
    "print(f\"\\nPass 1 - Benign: {benign_count_pass1}, Malignant: {malignant_count_pass1}\")\n",
    "print(f\"Pass 2 - Benign: {benign_count_pass2}, Malignant: {malignant_count_pass2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a consistent size\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(),  # Random vertical flip\n",
    "    transforms.RandomRotation(30),  # Random rotation for variety\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Color jitter for diversity\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Testing Transformations\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a consistent size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets for Pass 1 and Pass 2\n",
    "train_pass1_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PASS1,  # CSV for Pass 1 with 12k benign + all malignant\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "train_pass2_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PASS2,  # CSV for Pass 2 with the remaining 12k benign + all malignant\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "test_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TEST_CSV_PATH,  # Test CSV with augmented malignant and original benign samples\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=test_transforms,\n",
    "    is_test=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "# Create datasets\n",
    "\"\"\"\n",
    "train_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PATH,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False  # Indicates that this dataset is for training\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TEST_CSV_PATH,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=test_transforms,\n",
    "    is_test=True  # Indicates that this dataset is for testing\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "lr = 1e-5\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "num_workers=4\n",
    "\n",
    "# Determine if CUDA is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "print(f\"[INFO] Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloaders, lossFunc, Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights for Pass 1 dataset\n",
    "malignant_count_pass1 = len(train_pass1_dataset.metadata[train_pass1_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count_pass1 = len(train_pass1_dataset.metadata[train_pass1_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count_pass1 = len(train_pass1_dataset)\n",
    "\n",
    "# Set weights: higher for minority class (for Pass 1)\n",
    "weights_pass1 = [\n",
    "    benign_count_pass1 / total_count_pass1 if label == 0 else malignant_count_pass1 / total_count_pass1\n",
    "    for label in train_pass1_dataset.metadata['target']\n",
    "]\n",
    "sampler_pass1 = WeightedRandomSampler(weights_pass1, len(weights_pass1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights for Pass 2 dataset\n",
    "malignant_count_pass2 = len(train_pass2_dataset.metadata[train_pass2_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count_pass2 = len(train_pass2_dataset.metadata[train_pass2_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count_pass2 = len(train_pass2_dataset)\n",
    "\n",
    "# Set weights: higher for minority class (for Pass 2)\n",
    "weights_pass2 = [\n",
    "    benign_count_pass2 / total_count_pass2 if label == 0 else malignant_count_pass2 / total_count_pass2\n",
    "    for label in train_pass2_dataset.metadata['target']\n",
    "]\n",
    "sampler_pass2 = WeightedRandomSampler(weights_pass2, len(weights_pass2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\n",
    "# Compute weights for each class\n",
    "malignant_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count = len(train_dataset)\n",
    "\n",
    "# Set weights: higher for minority class\n",
    "weights = [benign_count / total_count if label == 0 else malignant_count / total_count for label in train_dataset.metadata['target']]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "When using a sampler, you should remove the shuffle argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoaders for Pass 1, Pass 2, and Test dataset\n",
    "# Create DataLoaders with weighted sampler for Pass 1 and Pass 2\n",
    "train_pass1_loader = DataLoader(\n",
    "    dataset=train_pass1_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_pass1,  # Use weighted random sampler for class balancing\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "train_pass2_loader = DataLoader(\n",
    "    dataset=train_pass2_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_pass2,  # Use weighted random sampler for class balancing\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create DataLoader instances\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m----> 3\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler, \u001b[38;5;66;03m# Set weights: higher for minority class\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#shuffle=True,  # Shuffle the data for training \u001b[39;00m\n\u001b[1;32m      7\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers,  \u001b[38;5;66;03m# Number of workers for data loading\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39mPIN_MEMORY  \u001b[38;5;66;03m# Use pin memory if using CUDA\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     12\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39mPIN_MEMORY  \u001b[38;5;66;03m# Use pin memory if using CUDA\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    #sampler=sampler, # Set weights: higher for minority class\n",
    "    shuffle=True,  # Shuffle the data for training # When using a sampler, \n",
    "    # I should remove the shuffle argument.\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps per epoch\n",
    "trainSteps1 = len(train_pass1_loader)\n",
    "trainSteps2 = len(train_pass2_loader)\n",
    "\n",
    "testSteps = len(test_loader)\n",
    "\n",
    "print(f\"[INFO] Training steps per epoch: {trainSteps1}\")\n",
    "print(f\"[INFO] Training steps per epoch: {trainSteps2}\")\n",
    "print(f\"[INFO] Testing steps per epoch: {testSteps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Initialize Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' # old approach\n",
    "# Calculate class weights for weighted cross entropy\n",
    "benign_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "\n",
    "malignant_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "\n",
    "# Adjust weight for handling class imbalance\n",
    "pos_weight = torch.tensor([benign_count / malignant_count], dtype=torch.float).to(DEVICE)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, where we pass the number of metadata features (3 in this case)\n",
    "num_metadata_features = 3  # Number of metadata features: sex, age, and site\n",
    "model = ResNetModel(num_metadata_features).to(DEVICE)\n",
    "\n",
    "# Loss function and optimizer #old lossFunc\n",
    "#lossFunc = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "lossFunc = FocalLoss(alpha=3, gamma=2)  # Adjust alpha to give more weight to malignant class\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#old scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Training History Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Loop Implementation with Metric Tracking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define the parameters for the training and test loop\n",
    "H = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with Pass 1 Dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_and_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train on Pass 1 Dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training with Pass 1 Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m H_pass1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_pass1_loader,\n\u001b[1;32m      6\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      9\u001b[0m     lossFunc\u001b[38;5;241m=\u001b[39mlossFunc,\n\u001b[1;32m     10\u001b[0m     DEVICE\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m     11\u001b[0m     NUM_EPOCHS\u001b[38;5;241m=\u001b[39mNUM_EPOCHS\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_and_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Train on Pass 1 Dataset\n",
    "print(\"Starting training with Pass 1 Dataset\")\n",
    "H_pass1 = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_pass1_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n",
    "# After training, we can use H for further analysis or plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Pass 2 Dataset\n",
    "print(\"Starting training with Pass 2 Dataset\")\n",
    "H_pass2 = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_pass2_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training loop ends, save the model\n",
    "# Ensure the directory exists, if not, create it\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "# Define the model filename with the .pth extension\n",
    "model_filename = \"melanoma_trained_model.pth\"\n",
    "\n",
    "# Full path to save the model\n",
    "model_save_path = os.path.join(MODEL_PATH, mod el_filename)\n",
    "\n",
    "# Save the model, replacing if it already exists\n",
    "if os.path.exists(model_save_path):\n",
    "    os.remove(model_save_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics for combined Pass 1 and Pass 2\n",
    "def plot_combined_metrics(H_pass1, H_pass2):\n",
    "    # Combine metrics from Pass 1 and Pass 2\n",
    "    combined_H = {}\n",
    "    for key in H_pass1:\n",
    "        combined_H[key] = H_pass1[key] + H_pass2[key]  # Concatenate metrics for Pass 1 and Pass 2\n",
    "    \n",
    "    # Number of epochs for combined training\n",
    "    epochs = range(1, len(combined_H[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 20))\n",
    "\n",
    "    # Plot Training and Test Loss\n",
    "    plt.subplot(4, 2, 1)\n",
    "    plt.plot(epochs, combined_H[\"train_loss\"], 'b', label='Train Loss')\n",
    "    plt.plot(epochs, combined_H[\"test_loss\"], 'r', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Accuracy\n",
    "    plt.subplot(4, 2, 2)\n",
    "    plt.plot(epochs, combined_H[\"train_acc\"], 'b', label='Train Accuracy')\n",
    "    plt.plot(epochs, combined_H[\"test_acc\"], 'r', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Precision\n",
    "    plt.subplot(4, 2, 3)\n",
    "    plt.plot(epochs, combined_H[\"train_precision\"], 'b', label='Train Precision')\n",
    "    plt.plot(epochs, combined_H[\"test_precision\"], 'r', label='Test Precision')\n",
    "    plt.title('Training and Test Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Recall\n",
    "    plt.subplot(4, 2, 4)\n",
    "    plt.plot(epochs, combined_H[\"train_recall\"], 'b', label='Train Recall')\n",
    "    plt.plot(epochs, combined_H[\"test_recall\"], 'r', label='Test Recall')\n",
    "    plt.title('Training and Test Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test F1 Score\n",
    "    plt.subplot(4, 2, 5)\n",
    "    plt.plot(epochs, combined_H[\"train_f1\"], 'b', label='Train F1 Score')\n",
    "    plt.plot(epochs, combined_H[\"test_f1\"], 'r', label='Test F1 Score')\n",
    "    plt.title('Training and Test F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test ROC AUC\n",
    "    plt.subplot(4, 2, 6)\n",
    "    plt.plot(epochs, combined_H[\"train_roc_auc\"], 'b', label='Train ROC AUC')\n",
    "    plt.plot(epochs, combined_H[\"test_roc_auc\"], 'r', label='Test ROC AUC')\n",
    "    plt.title('Training and Test ROC AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Precision-Recall Curve for each epoch (from Pass 1 and Pass 2)\n",
    "    plt.subplot(4, 2, 7)\n",
    "    for i, (precision, recall) in enumerate(combined_H[\"test_precision_recall_curve\"]):\n",
    "        plt.plot(recall, precision, label=f'Epoch {i+1}')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Average Precision Score\n",
    "    plt.subplot(4, 2, 8)\n",
    "    plt.plot(epochs, combined_H[\"test_average_precision\"], 'b', label='Test Average Precision')\n",
    "    plt.title('Average Precision Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the combined metrics for Pass 1 and Pass 2\n",
    "plot_combined_metrics(H_pass1, H_pass2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the plot to the specified path\n",
    "plot_filename = os.path.join(MODEL_PATH, \"training_metrics_plot.png\")\n",
    "\n",
    "# Use a higher DPI for better quality\n",
    "plt.savefig(plot_filename, format='png', dpi=300)  # Save the plot with higher DPI for clarity\n",
    "print(f\"Plot saved to {plot_filename}\")\n",
    "    \n",
    "plt.close()  # Ensure the plot is cleared after saving to avoid showing it blank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally prediction on lesions (Benign/Malignant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "test_metadata_df = LesionPredictions.load_metadata(TEST_CSV_PATH)\n",
    "\n",
    "# Make sure to define 'model', 'test_loader', and 'DEVICE'\n",
    "visualizer = LesionPredictions(model, test_loader, DEVICE)\n",
    "visualizer.inference_prediction()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
