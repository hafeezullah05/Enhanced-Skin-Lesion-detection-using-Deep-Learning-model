{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_test_loop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_and_test\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minteractive_visual_comparison\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_metadata, interactive_visual_comparison\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFocalLoss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FocalLoss\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#import torch.profiler\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#from torch.utils.tensorboard import SummaryWriter\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#from tensorboardX import SummaryWriter\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#from torch.profiler import profile, ProfilerActivity\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#import tkinter as tk\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#from tkinter import filedialog, messagebox\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Thesis_Hafeez/Thesis_Code/Enhanced-Skin-Lesion-detection-using-Deep-Learning-model/code/FocalLoss.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFocalLoss\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(FocalLoss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from natsort import natsorted # type: ignore\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import precision_score as skl_precision_score\n",
    "from sklearn.metrics import recall_score as skl_recall_score\n",
    "from sklearn.metrics import f1_score as skl_f1_score\n",
    "from sklearn.metrics import accuracy_score as skl_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import the code from all .py files\n",
    "\n",
    "from custom_dataset import CustomMelanomaDataset  # Import the custom dataset\n",
    "from resnet_model import ResNetModel\n",
    "from preprocessing_csv import PreprocessingCSV\n",
    "from train_test_loop import train_and_test\n",
    "from lesion_predictions import LesionPredictions\n",
    "from FocalLoss import FocalLoss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import torch.profiler\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "#from torch.profiler import profile, ProfilerActivity\n",
    "#import tkinter as tk\n",
    "#from tkinter import filedialog, messagebox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic root path\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "root_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "csv_path = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Display the structure of the dataset\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the universal path handling logic\n",
    "BASE_DIR = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "SPLIT_CSV_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv')\n",
    "\n",
    "# Universal Path Setup for Images\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'Train_JPEG', 'JPEG')\n",
    "\n",
    "# Paths for Train/Test CSVs\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'train_split.csv')\n",
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "\n",
    "# Create the full path to the CSV file\n",
    "CSV_PATH = os.path.join(root_path, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth.csv')\n",
    "preprocess_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez/Dataset/Train_JPEG/ISIC_2020_Training_GroundTruth_preprocess.csv')\n",
    "\n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez',  'Thesis_Code/Enhanced-Skin-Lesion-detection-using-Deep-Learning-model/results', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step : Create an instance of PreprocessingCSV\n",
    "preprocessor = PreprocessingCSV(CSV_PATH, BASE_DIR)\n",
    "\n",
    "# Step : Execute the preprocessing steps\n",
    "preprocessor.analyze_raw_data()\n",
    "preprocessor.check_for_anomalies()\n",
    "preprocessor.clean_data()\n",
    "preprocessor.save_clean_data()\n",
    "preprocessor.split_by_patient_id()\n",
    "preprocessor.verify_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transforms for image augmentation and normalization\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size (224x224)\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally for data augmentation\n",
    "    transforms.RandomVerticalFlip(),    # Randomly flip the image vertically\n",
    "    transforms.ToTensor(),  # Convert PIL Image or numpy.ndarray to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images with pre-defined mean and std\n",
    "])\n",
    "\n",
    "# Define transformations for testing\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size (224x224)\n",
    "    transforms.ToTensor(),  # Convert PIL Image or numpy.ndarray to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images with pre-defined mean and std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "# Create datasets\n",
    "train_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TRAIN_CSV_PATH,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=train_transforms,\n",
    "    is_test=False  # Indicates that this dataset is for training\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = CustomMelanomaDataset(\n",
    "    csv_file=TEST_CSV_PATH,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    transform=test_transforms,\n",
    "    is_test=True  # Indicates that this dataset is for testing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "lr = 1e-5\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "num_workers=4\n",
    "\n",
    "# Determine if CUDA is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "print(f\"[INFO] Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloaders, lossFunc, Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights for each class\n",
    "malignant_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "benign_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "total_count = len(train_dataset)\n",
    "\n",
    "# Set weights: higher for minority class\n",
    "weights = [benign_count / total_count if label == 0 else malignant_count / total_count for label in train_dataset.metadata['target']]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "When using a sampler, you should remove the shuffle argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create DataLoader instances\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m----> 3\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler, \u001b[38;5;66;03m# Set weights: higher for minority class\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#shuffle=True,  # Shuffle the data for training \u001b[39;00m\n\u001b[1;32m      7\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers,  \u001b[38;5;66;03m# Number of workers for data loading\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39mPIN_MEMORY  \u001b[38;5;66;03m# Use pin memory if using CUDA\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     12\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39mPIN_MEMORY  \u001b[38;5;66;03m# Use pin memory if using CUDA\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler, # Set weights: higher for minority class\n",
    "    #shuffle=True,  # Shuffle the data for training # When using a sampler, \n",
    "    # I should remove the shuffle argument.\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    "    pin_memory=PIN_MEMORY  # Use pin memory if using CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps per epoch\n",
    "trainSteps = len(train_loader)\n",
    "testSteps = len(test_loader)\n",
    "\n",
    "print(f\"[INFO] Training steps per epoch: {trainSteps}\")\n",
    "print(f\"[INFO] Testing steps per epoch: {testSteps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Initialize Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "# Calculate class weights for weighted cross entropy\n",
    "benign_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'benign'])\n",
    "\n",
    "malignant_count = len(train_dataset.metadata[train_dataset.metadata['benign_malignant'] == 'malignant'])\n",
    "\n",
    "# Adjust weight for handling class imbalance\n",
    "pos_weight = torch.tensor([benign_count / malignant_count], dtype=torch.float).to(DEVICE)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, where we pass the number of metadata features (3 in this case)\n",
    "num_metadata_features = 3  # Number of metadata features: sex, age, and site\n",
    "model = ResNetModel(num_metadata_features).to(DEVICE)\n",
    "\n",
    "# Loss function and optimizer #old lossFunc\n",
    "#lossFunc = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "lossFunc = FocalLoss(alpha=3, gamma=2)  # Adjust alpha to give more weight to malignant class\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#old scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler\n",
    "# Add a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Training History Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training Loop Implementation with Metric Tracking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the training and test loop\n",
    "H = train_and_test(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lossFunc=lossFunc,\n",
    "    DEVICE=DEVICE,\n",
    "    NUM_EPOCHS=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "# After training, we can use H for further analysis or plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training loop ends, save the model\n",
    "# Ensure the directory exists, if not, create it\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "# Define the model filename with the .pth extension\n",
    "model_filename = \"melanoma_trained_model.pth\"\n",
    "\n",
    "# Full path to save the model\n",
    "model_save_path = os.path.join(MODEL_PATH, model_filename)\n",
    "\n",
    "# Save the model, replacing if it already exists\n",
    "if os.path.exists(model_save_path):\n",
    "    os.remove(model_save_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(H):\n",
    "    epochs = range(1, len(H[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 20))\n",
    "\n",
    "    # Plot Training and Test Loss\n",
    "    plt.subplot(4, 2, 1)\n",
    "    plt.plot(epochs, H[\"train_loss\"], 'b', label='Train Loss')\n",
    "    plt.plot(epochs, H[\"test_loss\"], 'r', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Accuracy\n",
    "    plt.subplot(4, 2, 2)\n",
    "    plt.plot(epochs, H[\"train_acc\"], 'b', label='Train Accuracy')\n",
    "    plt.plot(epochs, H[\"test_acc\"], 'r', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Precision\n",
    "    plt.subplot(4, 2, 3)\n",
    "    plt.plot(epochs, H[\"train_precision\"], 'b', label='Train Precision')\n",
    "    plt.plot(epochs, H[\"test_precision\"], 'r', label='Test Precision')\n",
    "    plt.title('Training and Test Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test Recall\n",
    "    plt.subplot(4, 2, 4)\n",
    "    plt.plot(epochs, H[\"train_recall\"], 'b', label='Train Recall')\n",
    "    plt.plot(epochs, H[\"test_recall\"], 'r', label='Test Recall')\n",
    "    plt.title('Training and Test Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test F1 Score\n",
    "    plt.subplot(4, 2, 5)\n",
    "    plt.plot(epochs, H[\"train_f1\"], 'b', label='Train F1 Score')\n",
    "    plt.plot(epochs, H[\"test_f1\"], 'r', label='Test F1 Score')\n",
    "    plt.title('Training and Test F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Test ROC AUC\n",
    "    plt.subplot(4, 2, 6)\n",
    "    plt.plot(epochs, H[\"train_roc_auc\"], 'b', label='Train ROC AUC')\n",
    "    plt.plot(epochs, H[\"test_roc_auc\"], 'r', label='Test ROC AUC')\n",
    "    plt.title('Training and Test ROC AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Precision-Recall Curve\n",
    "    plt.subplot(4, 2, 7)\n",
    "    for i, (precision, recall) in enumerate(H[\"test_precision_recall_curve\"]):\n",
    "        plt.plot(recall, precision, label=f'Epoch {i+1}')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Average Precision Score\n",
    "    plt.subplot(4, 2, 8)\n",
    "    plt.plot(epochs, H[\"test_average_precision\"], 'b', label='Test Average Precision')\n",
    "    plt.title('Average Precision Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the metrics\n",
    "plot_metrics(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the plot to the specified path\n",
    "plot_filename = os.path.join(MODEL_PATH, \"training_metrics_plot.png\")\n",
    "\n",
    "# Use a higher DPI for better quality\n",
    "plt.savefig(plot_filename, format='png', dpi=300)  # Save the plot with higher DPI for clarity\n",
    "print(f\"Plot saved to {plot_filename}\")\n",
    "    \n",
    "plt.close()  # Ensure the plot is cleared after saving to avoid showing it blank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally prediction on lesions (Benign/Malignant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'Thesis_Hafeez', 'Dataset', 'split_csv', 'test_split.csv')\n",
    "test_metadata_df = LesionPredictions.load_metadata(TEST_CSV_PATH)\n",
    "\n",
    "# Make sure to define 'model', 'test_loader', and 'DEVICE'\n",
    "visualizer = LesionPredictions(model, test_loader, DEVICE)\n",
    "visualizer.inference_prediction()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
